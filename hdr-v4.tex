%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% LaTeX book template                           %%
%% Author:  Amber Jain (http://amberj.devio.us/) %%
%% License: ISC license                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%!TEX TS-program = pdfLaTeX
%!TEX encoding = IsoLatin
%!TEX spellcheck = fr-FR

\documentclass[12pt,twoside,openright]{book}

%\usepackage{soul}

%\documentclass[a4paper,11pt]{book}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[francais]{babel}

\usepackage[usenames,dvipsnames]{color}

\usepackage{lmodern}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Source: http://en.wikibooks.org/wiki/LaTeX/Hyperlinks %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{pdfpages}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{a4}
\usepackage{indentfirst}
\usepackage{fancyhdr}
\usepackage{varioref}
\usepackage{makeidx}

\usepackage{mslapa}

\usepackage{ulem}

%\usepackage{apacite}
%\usepackage[longnamesfirst,nonamebreak]{natbib}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagestyle{fancy}

\addtolength{\headwidth}{\marginparsep}
\addtolength{\headwidth}{\marginparwidth}
\renewcommand{\chaptermark}[1]{\markboth{#1}{}}
\renewcommand{\sectionmark}[1]{\markright{\thesection\ #1}}
\fancyhf{}
\fancyhead[LE,RO]{\bfseries\thepage}
\fancyhead[LO]{\bfseries\rightmark}
\fancyhead[RE]{\bfseries\leftmark}
\fancypagestyle{plain}{
\fancyhead{} % get rid of headers
\renewcommand{\headrulewidth}{0pt} % and the line
}


%% Apalike hyphenation %%%
%\let\oldbibitem=\bibitem
%\renewcommand{\bibitem}[2][]{\oldbibitem[#1]{#2}\newline}

%%% Margins %%%
\voffset -1.04cm
\textheight 23cm
\hoffset -1in
\evensidemargin 2.5cm
\oddsidemargin 2.5cm
\textwidth 16cm

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\textwidth16cm
%\textheight23cm
%\oddsidemargin0,5cm
%\evensidemargin0,5cm
%\topmargin-1cm
%\parskip0,5cm
%\parindent0cm



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 'dedication' environment: To add a dedication paragraph at the start of book %
% Source: http://www.tug.org/pipermail/texhax/2010-June/015184.html            %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newenvironment{dedication}
{
   \cleardoublepage
   \thispagestyle{empty}
   \vspace*{\stretch{1}}
   \hfill\begin{minipage}[t]{0.66\textwidth}
   \raggedright
}
{
   \end{minipage}
   \vspace*{\stretch{3}}
   \clearpage
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Chapter quote at the start of chapter        %
% Source: http://tex.stackexchange.com/a/53380 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\makeatletter
\renewcommand{\@chapapp}{}% Not necessary...
\newenvironment{chapquote}[2][2em]
  {\setlength{\@tempdima}{#1}%
   \def\chapquote@author{#2}%
   \parshape 1 \@tempdima \dimexpr\textwidth-2\@tempdima\relax%
   \itshape}
  {\par\normalfont\hfill--\ \chapquote@author\hspace*{\@tempdima}\par\bigskip}
\makeatother


% Babel ``Sommaire'' à la place de ``table des matières''
\renewcommand{\contentsname}{Sommaire}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% First page of book which contains 'stuff' like: %
%  - Book title, subtitle                         %
%  - Book author name                             %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Book's title and subtitle
\title{\Huge \textbf{Apprentissage dans les architectures cognitives}  \\ \vspace{1cm} \huge Contributions pour l'informatique et les neurosciences \\ \vspace{1cm}}
% Author
\author{\textsc{Emmanuel Daucé}}%\thanks{\url{www.example.com}}}


\begin{document}

\frontmatter
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Add a dedication paragraph to dedicate your book to someone %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{dedication}
%Dedicated to Calvin and Hobbes.
%\end{dedication}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Auto-generated table of contents, list of figures and list of tables %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\tableofcontents
%\listoffigures
%\listoftables

\mainmatter

%\section*{Remerciements}
%\begin{itemize}
%\item A special word of thanks goes to Professor Don Knuth\footnote{\url{http://www-cs-faculty.stanford.edu/~uno/}} (for \TeX{}) and Leslie Lamport\footnote{\url{http://www.lamport.org/}} (for \LaTeX{}).
%\item I'll also like to thank Gummi\footnote{\url{http://gummi.midnightcoding.org/}} developers and LaTeXila\footnote{\url{http://projects.gnome.org/latexila/}} development team for their awesome \LaTeX{} editors.
%\item I'm deeply indebted my parents, colleagues and friends for their support and encouragement.
%\end{itemize}
%\mbox{}\\
%\mbox{}\\
%\noindent Amber Jain \\
%\noindent \url{http://amberj.devio.us/}

%%%%%%%%%%%%%%%%
% NEW CHAPTER! %
%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%11
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%                                            ##                                                               %%% 
%%%                                          ####                                                               %%%  
%%%                                            ##                                                               %%% 
%%%                                            ##                                                               %%% 
%%%                                            ##                                                               %%%    
%%%                                            ##                                                               %%% 
%%%                                          ######                                                             %%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Introduction}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%\begin{chapquote}{Author's name, \textit{Source of this quote}}
%``This is a quote and I don't know who said this.''
%\end{chapquote}

% le modèle de Hopfield
% Qu'est-ce que le chaos?

% Qu'est-ce qu'un système apprenant

Le but de ce document est de fournir un aperçu étendu des questions et des approches que j'ai 
eu l'occasion d'aborder au cours des vingts dernières années, depuis mes premiers pas en recherche. 
{\color{Orange} Ce document est organisé autour des différentes contributions et productions scientifiques auxquelles
j'ai participé, dans un ordre non chronologique mais thématique.
A l'issue de ce parcours, il fournit également un aperçu d'un certain nombre de questions 
restées ouvertes, et pouvant faire l'objet de développements futurs.}

Ces différentes contributions appartiennent à un champ de recherche assez large à l'intersection
des neurosciences et de l'informatique, généralement appelé ``\textit{neurosciences computationnelles}''.
Le terme computationnel fait référence à la notion de calcul. On peut donc également parler de neuro-calcul,
bien que ce terme soit peu usité\footnote{  
sans doute pour ne pas le confondre avec le bio-calcul, 
qui concerne les opérations réalisées au niveau moléculaire, autrement dit
le fonctionnement de la cellule en tant que machine.}
\emph{Les neurosciences computationnelles 
visent donc à identifier ce qui, dans l'activité des neurones, s'apparente à un calcul; à identifier 
le type et la nature des opérations réalisées par le système nerveux}. 

{\color{Cyan}
Mon parcours en résumé...

L'organisation du document...
}

\section{Neurosciences computationnelles}

Si les principes de fonctionnement élémentaires du système nerveux central (neurones, synapses) 
commencent à être bien connus, 
le fonctionnement intégré du système dans son ensemble reste l'objet de nombreuses conjectures.
Les modèles mathématiques et les simulations informatiques permettent de proposer des hypothèses 
de fonctionnement et/ou 
de valider certaines hypothèses issues de l'observation. 
{\color{Orange} Les neurosciences computationnelles sont ainsi la branche de l'{\bf !!informatique!!} qui, d'une part, élabore des modèles de calcul 
	inspirés par les hypothèses proposées en neurosciences, et, d'autre part, propose des interprétations
	de données biologiques à partir de principes et modèles issus des sciences de l'information.}
Ainsi, l'étude du système nerveux, sous l'angle computationnel, a deux objectifs:
\begin{enumerate} 
\item explorer des mécanismes de calcul alternatifs 
\item comprendre le fonctionnement du cerveau
\end{enumerate}

\paragraph{Mécanismes de calcul alternatifs} Le premier objectif est
de s'inspirer du fonctionnement du système nerveux pour découvrir de nouveaux principes pour le calcul artificiel,
ou de nouvelles architectures, pour résoudre des problèmes qui résistent encore aux calculateurs actuels.
Mieux comprendre le cerveau, c'est donc potentiellement étendre le domaine de la science informatique,
étendre le type de données qu'elles sont capables de traiter (son, langage, images, ...), 
augmenter leur capacité à contrôler leur environnement, 
les rendre plus proches, 
en terme de fonctionnement, des humains avec lesquels elles interagissent. 

\paragraph{Fonctionnement du cerveau} Le deuxième objectif consiste 
à utiliser les connaissance informatiques et mathématiques actuelles pour mieux comprendre le système nerveux.
Il s'agit d'une certaine façon d'identifier ce qui dans l'activité nerveuse s'apparente aux algorithmes
utilisés dans des tâches computationnelles variées, comme le contrôle, l'apprentissage, le traitement des images et des données;
voir si les solutions mises au point dans des contextes d'ingénierie proches de celui face auquel se trouve
le système nerveux ont des équivalents au sein de l'activité nerveuse (ou si, au contraire, la nature 
utilise des principes et méthodes différents). L'objectif est, à partir des observations, de mieux 
identifier les rôles et les interactions à l'intérieur du système nerveux, et potentiellement
de mieux soigner les désordres et maladies neurologiques. 

{\color{Orange} Cette approche a environ une soixantaine d'années d'existence, {\bf !!!FLOU!!!et se confond parfois avec} 
les sciences cognitives
(étude des opérations de haut niveau réalisées par le cerveau), 
les neurosciences fonctionnelles (étude des rôles des différentes structures anatomiques), 
et l'apprentissage automatique (programmes s'adaptant aux données).}

{\color{Orange} Dans le cadre des neurosciences computationnelles, la question n'est pas nécessairement de
produire le meilleur algorithme, ou de résoudre une nouvelle tâche, mais de produire des
modèles permettant de mieux comprendre les opérations réalisées par le cerveau, ou de prendre
exemple sur des architectures ou des circuits neuronaux réels pour valider ou invalider des hypothèses
liées aux types d'opérations produites par le système nerveux.}



\subsection{Une nouvelle discipline?}



{\color{Purple} 
Les neurosciences computationnelles sont par définition pluridisciplinaires. 
\begin{itemize}
\item Elles reposent en premier lieu sur le 
champ des \textit{neurosciences}, qui ont connu un développement considérables au cours des quarante dernières années,
avec l'apparition de nouvelles techniques de mesure permettant d'observer le cerveau en fonctionnement à
différentes échelles. 
\item Elles reposent en second lieu sur la \textit{modélisation mathématique et informatique}, qui a également
connu un développement important lié à l'augmentation des capacités de calcul des ordinateurs, ainsi que de la
capacité à traiter des données massives à travers des techniques dites d'apprentisage automatique (\textit{Machine 
learning}). 
\item Un dernier champ de recherche, celui de la \textit{robotique autonome} et du contrôle moteur, est également impliqué, dans la mesure où le
développement d'interfaces bioniques nécessite une meilleure connaissance des mécanismes de 
contrôle à l'oeuvre chez l'homme et l'animal.
\end{itemize}
}

\paragraph{Trois niveaux d'analyse}
Selon la typologie classique proposée par David Marr \shortcite{Marr82}, 
un modèle peut appartenir à différents niveaux de généralité, à savoir le niveau computationnel (la 
logique), 
le niveau algorithmique (encodage et opérations réalisées), et
la réalisation matérielle (le réseau de neurones ou ``substrat'').   

Cette distinction en trois niveaux permet principalement de distinguer ce qui relève de la définition du problème~:
définition de la tâche, en fonction des contraintes physiques et de l'appareillage sensoriel et moteur, d'une part, et
d'autre part ce qui relève de la réalisation (algorithmique) de la tâche, c'est à dire principalement le circuit de 
traitement. C'est, en terminologie informatique, la distinction entre
l'équation à résoudre, la brique logicielle qui la résout et le circuit logique qui l'implémente.
Marr postule une relative indépendance entre les différents niveaux de description, en établissant une
claire séparation entre le substrat neuronal, le circuit de traitement et l'espace de la tâche. 

{\color{Orange}{\bf !!
	Cette séparation correspond assez bien au problème des disparités d'échelle 
	évoqué plus haut.!!} La contrainte physique du corps, engagé dans différentes tâches, à des échelles
spatiales macroscopiques et des échelles temporelles lentes d'une part.
La contrainte du circuit microscopique, constitué d'unité neuronales échangeant rapidement des signaux discrets
d'autre part.
Entre les deux, un niveau architectural, constitué de circuits imbriqués intégrant l'activité de
larges portion du cerveau, 
constituant un niveau de description relativement indépendant des deux autres.}

%XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
\paragraph{Approche ascendante (ou ``bottom-up'')}
La méthodologie ``bottom-up'' consiste à étudier les
conditions d'émergence de fonctions de haut niveau à partir 
des caractéristiques d'un réseau de neurones plongé dans un
environnement contraignant.
Les réseaux de neurones sont en effet capables de manifester des formes d'organisation spontanée~: comportement critique, 
transitions entre différentes configurations d'activité, activité persistante et mémoire 
à court terme (rémanence) etc.
L'ajout de mécanismes de plasticité synaptique vise à exploiter 
cette richesse intrinsèque 
pour que le réseau puisse acquérir des connaissances et des compétences nouvelles.
{\color{Purple} 
A partir des constituants de la dynamique, de leurs propriétés (leur ``expressivité'') cette approche vise à 
établir les propriétés ou les fonctions qui peuvent être attendues de l'assemblage des constituants, autrement dit établir la 
``puissance'' d'un langage qui utiliserait 
l'activité des neurones comme syntaxe de base.
}

\paragraph{Approche descendance (ou ``top-down'')}
La méthodologie ``top-down'' consiste à étudier les conditions de réalisation
d'une certaine tâche par étapes descendantes, de la contrainte générale du corps engagé dans la tâche
à la réalisation matérielle du circuit qui la résout.
L'approche descendante consiste souvent à identifier les aspects non-triviaux
d'une tâche. L'étude des temps de réponse, des courbes d'apprentissage et des
taux d'erreur permet d'explorer les caractéristiques limites d'un comportement et de mieux appréhender
les contraintes posées par système qui l'implémente physiquement.
Ce peuvent être des contraintes physiques (limites d'extension, de course, ...) mais
également des limites provenant des caractéristiques physiques du réseau de neurones sous-jacent.
L'élaboration d'un modèle descriptif prenant en compte plusieurs échelles permet alors d'émettre des hypothèses 
sur les sources (physiques ou logiques) des limitations comportementales observées. 

{\color{Violet}
\subsection{Développement}
Les neurosciences computationnelles se sont fortement développées, et ont commencé à
s'organiser en tant que champ de recherche, à partir de la fin des années 90, avec le soutien de plusieurs programmes 
nationaux et internationaux. On citera, parmi les plus actifs, le réseau
Bernstein pour les neurosciences computationnelles en Allemagne, développé depuis 2004, 
le collectif INSC (International Neuroinformatics Coordination Facility) qui tente de coordonner les 
nombreux projets de recherche de la discipline au niveau international (basé en Suède). Elles ont 
bénéficié de nombreux soutiens financiers européens (NEST, Brain scales, Blue Brain project, ...)
qui se sont récemment concrétisés par l'attribution d'un financement 'Flagship' pour le projet ``Human Brain
Project''  qui regroupe les principaux acteurs des projets précédents. 
Ce champ de recherche commence également à s'insérer dans les programmes de deuxième cycle, comme le montrent les
nombreux cours en ligne disponibles sur le Web
(voir par exemple ceux de Sébastien Seung au MIT, Andrew Ng à l'Université de Washington, etc.)

Au niveau des revues scientifiques, les neurosciences computationnelles ne bénéficient pas de la même
couverture académique que les disciplines plus anciennement implantées. 
Les revues qui ont historiquement participé à son développement sont les revues traitant des 
réseaux de neurones artificiels apparues à la fin des années 80 (Neural Networks, Neural 
Computation, Neural Computing Letters, Biogical Cybernetics, etc.).
Certaine revues plutôt dédiées au traitement du signal, comme NeuroImage, acceptent également des papiers du domaine.
Enfin, les revues de physique (Physica-D) hébergent également des papiers traitant des neurosciences computationnelles.  
Suite au développement du champ de recherche et de sa couverture relativement lacunaire par les revues scientifiques, 
sont apparues des revues en ``Open Acces'' spécialement dédiées au domaine. 
On notera en particulier PLOS-Computational Biology et Frontiers in Computational Neurosciences qui ont aidé à compenser
la relatif manque de visibilité du domaine.
Il existe enfin quelques conférences spécifiquement dédiées à ce champ de recherche, comme la conférence CNS (Computational
Neurosciences) et la conférence CoSyNe (Computational and Systems Neuroscience). Dans le domaine de la robotique bio-inspirée,
on peut citer la conférence SAB (Simulation of Adaptive Behavior) qui a eu un fort impact au début des années 2000 mais a 
fini par disparaître faute d'audience suffisante. 

\subsection{Les neurosciences computationnelles en France}
%Voir : https://web.archive.org/web/20131020013653/http://neurocomp.fr/

\subsection{Etat de l'art} Le champ de recherche s'organise autour de plusieurs pôles. 
(1) simulation neuro-réaliste : mieux comprendre le fonctionnement du neurone, et des interacions entre neurones. 
se cantonne parfois au simple neurone, à la stucture de la cellule. Modèles de plasticité synaptique, de la transmission
du signal nerveux, des échanges avec le milieu... (2) modèles conceptuels, principes de traitement de l'information,
codes neuronaux, mécanismes de mémoire et d'apprentissage;
(3) architectures neuronales, dynamique à large échelle, neurones à spikes, réseaux équilibrés (plutôt physique
théorique)...

Les principales avancées sont:
\begin{itemize}
 \item le système Visuel, Olshausen, les champs récepteurs, les architectures profondes, approche profonde; non-negative matrix factorization.
 \item dynamique stochastique, Fokker-Planck, etats de haute conductance, réseaux équilibrés
 \item généralisation des modèles physiques à l'analyse de l'imagerie... Modèles inverses. Friston
 \item STDP et codage par rang
 \item modèles de la décision et circuit de la récompense. Modèles du cervelet. Kalman.
 \item ce qui se fait en robotique. Franceschini. Japonais. MIT.
\end{itemize}


Problèmes rencontrés : foisonnement des modèles et des simulateurs informatiques. Difficulté à  reproduire les résultats. 
problème de la validation des modèles. Confrontation aux données empiriques. 

Il manque : des principes fondateurs, un langage commun, 

\section{Parcours personnel} 


\subsection{Les années 90}

Les voies d'entrée dans cette branche de la recherche sont multiples, puisqu'on y trouve à la fois des mathématiciens, 
des informaticiens, des physiciens, des biologistes, des médecins et des psychologues. En ce qui me concerne, 
j'ai suivi un parcours classique d'élève-ingénieur via le système des classes préparatoires et 
des concours de Grandes Ecoles. J'ai intégré l'Ecole Nationale Supérieure d'Electronique, d'Electrotechnique, 
d'Informatique, d'Hydraulique de Toulouse (ENSEEIHT) en Septembre 2002. 

Ma spécialisation vers les sciences cognitives s'est  tout d'abord faite avec la rencontre en début de troisième année avec Bernard
Doyon, médecin et chercheur à l'INSERM (Hôpital de Purpan) suite aux assises du Pôle de Recherche en Science 
Cognitives de Toulouse (PRESCOT), tenues à Labège, les 23 et 24 
septembre 1994. Bernard Doyon y présentait de façon très accessible l'étude de comportements chaotiques
obtenus par simulation de réseaux de neurones, en faisant le parallèle avec le fonctionnement du cerveau 
(en particulier avec l'étude des dimensions fractales des signaux électrophysiologiques pour différents états de veille
et de sommeil, développée à l'époque par l'équipe d'Agnès Babloyantz à Bruxelles, ainsi que les travaux
de Walter Freeman sur la réponse aus stimuli olfactifs sur des grilles d'électrodes chez le lapin).

Suite à une discussion, j'ai décidé de m'engager dans cette voie de recherche (simulation de réseaux de neurones
chaotiques) dans le cadre d'un stage du DEA de Representation de la Connaissance et Formalisation
du Raisonnement (RCFR), qui faisait partie des DEAs pouvant être suivis en parallèle de la troisième année d'études. 
Les cours étaient centrés sur la logique formelle, en particulier les grammaires génératives, 
les systèmes experts, les modèles linguistiques et la théorie de jeux. Les réseaux de neurones y étaient très peu voire 
pas abordés. 

Le stage était situé au sein du Département d'Etudes et de Recherche en Informatique (DERI), 
dans un laboratoire de l'Office Nationale de Recherche d'Etudes et de Recherche Aérospoatiales (ONERA) rattaché à 
l'Ecole Nationale Supérieure de l'Aéronautique et de l'espace (Sup'Aéro). 
J'ai intégré le petit groupe animé par Manuel Samuelidès, professeur à Supaéro, et Bernard Doyon, chercheur
à l'INSERM, autour des réseaux de neurones récurrents à connectivité aléatoire. Cette étude avait démarré 
au début des années 90.

Lors de mon arrivée, deux étudiants venaient de passer leur thèse sur ce thème.
D'un côté, Bruno Cessac, diplômé en physique théorique de l'université Paul sabatier, avait travaillé sur l'analyse en
champ moyen des réseaux de neurones récurrents aléatoires \cite{Ces95}. De l'autre, Mathias Quoy, diplômé de l'ENSEEIHT,
avait travaillé sur le comportement à taille finie (route vers le chaos) \cite{Doyon1993,CESSAC94}.
Mon rôle était alors principalement de continuer cette ligne de recherche,
et en particulier approfondir la question de la plasticité synaptique
et de l'apprentissage dans ce type de réseaux. 

Je connaissais alors peu de choses des systèmes dynamiques et du chaos, et mes connaissances en neurosciences se 
limitaient à une culture grand public, via quelques lectures. C'est donc principalement au travers 
de lectures de travaux scientifiques que je me suis formé à ces deux domaines.
L'ambiance de travail était très stimulante et nous avons rapidement pu produire 
de nouveaux résultats qui ont constitué mon rapport de fin d'études et de DEA \cite{Dau95}.
J'ai obteu mon diplôme d'ingénieur informaticien en juin 1995 et mon diplôme de DEA en septembre 1995. 
Ayant terminé à la deuxième place, j'ai pu bénéficier d'une bourse ministérielle qui m'a permis de 
poursuivre en thèse, toujours sous la direction de Bernard Doyon, au sein de l'équipe animée par Manuel 
Samuelidès au DERI.

Le sujet étudié en DEA, n'ayant pas été épuisé, est donc devenu mon sujet de thèse. 
Plus précisément, ma thèse portait sur l'étude d'un mécanisme de \textit{réduction} de la dynamique
par plasticité synaptique au sein de réseaux de neurones récurrents aléatoires.
Il s'agissait d'un sujet assez vaste, qui s'est progressivement orienté vers 
l'apprentissage et la reconnaisssance de séquences spatio-temporelles dans les réseaux de neurones récurrents
organisés en plusieurs couches de traitement. 

Cette thèse m'a également permis d'assister à l'essor du champ des neurosciences computationnelles au cours des 
années 1995-2000  (sans directement
travailler sur ces sujets), avec (1) les premiers neurones à impulsions (Thorpe-DYNN-PCNN)
(2) l'émergence du concept de connectivté fonctionnelle large échelle
(3) les modèles de champ neuronaux (Schöner), les concepts de champ récepteur

Dans le même temps, le champ des réseaux de neurones artificiels se rapproche de plus en plus de l'apprentissage 
statistique (qui deviendra le ``Machine Learning'') avec l'essor des machines à vecteurs supports (SVM) à la fin des
années 90.

Les résultats de la thèse ont donné lieu à de nombreuses publications et présentations en conférences. 
premier papier qui présente ces idées de la manière la plus complète est celui de Neural Networks, 
datant de 1998 \shortcite{Dau98A} ({\bf !! VOIR... !!}).
Il porte sur l'étude de la réaction (spontanée ou acquise) d'un réseau de neurones récurrent aléatoire
à des patrons (stimuli) également aléatoires.

% 1999 - Moynot - Pinaud - Samuelides

Une partie du travail, portant sur des architectures
multi-couches, a été réalisé en collaboration avec Olivier Moynot, étudiant diplômé de l'ENS Cachan, qui a entamé 
sa thèse sous la direction de Manuel Samuelidès un an après moi, et Olivier Pinaud, étudiant en de DEA 
de physique théorique, qui a effectué son stage à l'OBNERA au printemps 1998. 

{\color{Orange} En collaboration avec Olivier Moynot, Manuel Samuelides et Olivier Pinaud \shortcite{Dau99A,dauce01a}, j'ai également 
travaillé sur la définition et 
l'étude du comportement à taille finie d'un modèle de réseau de neurones aléatoire (RNA) multi-populations (voir section \ref{sec:balanced}).{\bf!! à développer!!}}

La troisième série de résultats de la thèse et des publication associées \shortcite{Dau00,Dau02} porte sur une architecture 
neuronale  permettant d'implémenter un modèle de la perception des signaux spatio-temporels.
Ces résultats reposent sur une architecture de réseaux récurrents aléatoires à plusieurs couches (deux ou trois).
Le formalisme multi-couches est le même que celui utilisé dans les réseaux équilibrés (voir section \ref{sec:balanced}).
Ici les couches se distinguent non par la nature de leurs neurones mais par le rôle qu'elles prennent dans le processus
d'apprentissage : une (ou deux) couche(s) primaire(s) ayant un rôle passif (sans dynamique intrinsèque)
et une couche secondaire (ou associative) possédant une dynamique intrinsèque, analogue aux réseaux récurents aléatoires
à une population.

L'étude du mécanisme de reconnaissance par ``réduction'' de la dynamique constitue le coeur
de ma thèse,
}

  



Les recherches que j'ai menées balayent un grand nombre de modèles, des plus détaillés ({\bf !!REF!! neurones à 
impulsion} \cite{gerstner02}, {\bf !!REF!! neurones à conductance}) \cite{HH52} 
aux plus grossiers (activités de {\bf !!REF!! masses neuronales} à l'échelle
macroscopique \cite{Wilson1972}, modèles basés sur des mesures psychophysiques). 
Différentes architectures et méthodes d'apprentissage sont proposées, reposant sur différents 
paradigmes ({\bf !!supervisé} \cite{Rosen58}, {\bf !!REF!! non supervisé} \cite{Koh82}, 
{\bf !!REF!! par renforcement} \cite{SUTTON98}), 
sur des tâches de contrôle en boucle ouverte ou
en boucle fermée, et s'inspirant à des degrés divers de l'architecture du système nerveux.

{\color{Orange} Ce rapport {\bf!!évite les détails mathématiques!!}, et insiste sur la mise en perspectives des travaux présentés
dans le cadre des problématiques des neurosciences computationnelles. 
Pour les figures, formules et illustrations, il faudra se reporter au document annexe contenant  une sélection
de papiers et de contributions à des conférences scientifiques.}


%%% Ici on va expliquer le vocabulaire de base : les syst dynamiques, la plasticité, l'apprentissage, le contrôle
%%% en boucle fermée

Ce document est organisé en quatre grandes parties. 
\begin{itemize}
 \item Dynamique des grands réseaux de neurones aléatoires
 \item Plasticité synaptique et apprentissage
 \item Architectures de contrôle
 \item Projet scientifique
\end{itemize}

Les trois premières parties présentent les résultats  



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%22
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%                                                 #######                                                     %%%
%%%                                                ##     ##                                                    %%%    
%%%                                                       ##                                                    %%%     
%%%                                                 #######                                                     %%%   
%%%                                                ##                                                           %%%
%%%                                                ##                                                           %%% 
%%%                                                #########                                                    %%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Dynamique des grands réseaux de neurones aléatoires}

Ce chapitre est consacré à l'analyse des capacités d'expression des grands réseaux de neurones. [REPRENDRE CHAPEAU]

\section{Neurones et réseau}

Inspirés par le fonctionnement du cerveau, les réseaux de neurones sont des modèles de calculateurs 
 dans lesquels le ``programme'' (la fonction de réponse) est décrite par un réseau.
Ce réseau est constitué d'un ensemble de noeuds, qui forment les unités de calcul, et un ensemble d'arêtes,  
pondérées et orientées, %définissant l'intensité du signal
qui transportent le signal entre les différentes unités de calcul.

Il existe de nombreux modèles de neurones et de réseaux de neurones. La modélisation des processus neuronaux
repose donc sur un choix du modélisateur, qui est fonction du mécanisme qu'il souhaite étudier, des outils
d'analyse et/ou de la puissance de calcul disponible(s). 

Je liste ci-dessous quelques propriétés et élements de vocabulaire communs à la plupart des modèles de réseaux de
neurones, indépendamment de leur utilisation en modélisation ou en apprentissage automatique. 

\subsection{Activité et signal}

Chaque unité de calcul (ou neurone) est modélisée comme une fonction de réponse $f$ qui 
traite un jeu de \textit{données d'entrée} multimodal $s^\text{in}_1, ..., s^\text{in}_n$.
On peut supposer sans perte de généralité que les données d'entrée sont indexées sur l'axe temporel.
On parle alors de \textit{signal d'entrée} où $s^\text{in}_i = \{s^\text{in}_i(t)\}_{t \in \{t_0,... t_f\}}$ 
représente un jeu de données indexé sur une trame temporelle et 
$s^\text{in}_i(t)$ représente le $i^\text{ème}$ signal au temps $t$.

La sortie du neurone au temps $t_f$ est un scalaire: 
\begin{align}\label{eq:neurone_s_out}
s^\text{out}(t_f) = f(s^\text{in}_1, ..., s^\text{in}_n)
\end{align}
(on parle aussi de \textit{réponse} du neurone aux signaux d'entrée).
Un neurone est donc une unité élémentaire de traitement des données. %Un neurone est implicitement une unitÃ© de traitement \textit{non-linÃ©aire}.

Le signal de sortie du neurone $s^\text{out} = \{s^\text{out}(t)\}_{t \in \{t_0,... t_f\}}$ est constituée 
d'une succession d'états ``hauts'' et d'états ``bas''.
Un neurone dont la sortie à l'instant $t$ est dans l'état haut est dit \textit{activé}. Un neurone 
dont la sortie à l'instant $t$ est dans l'état bas est dit \textit{inactif}. 

\emph{Exemples:}
\begin{itemize}
\item Dans le cas le plus simple des neurones binaires, 
le signal de sortie est une succession de 0 (inactif) ou de 1 (actif) \shortcite{MCu43}. 
\item Dans le cas plus complexe de neurones à impulsions, le signal de sortie est un train de potentiels d'action (PA), représenté par une 
somme de Diracs tels que $s(t) = \sum_{\hat{t} \in \mathcal{T}_\text{out}} \delta(t - \hat{t})$, où $\mathcal{T}_\text{out}$ est un ensemble
contenant les instants de décharge (voir par exemple \shortcite{gerstner02}).
%Dans le cas de neurones fréquentiels, le signal contient des grandeurs continues correspondant à la fréquence de décharge instantanée
%de la sortie.
\end{itemize}

Le signal de sortie (la suite d'états hauts et d'états bas) est aussi appelé l'\textit{activité} du neurone. Ce signal est transporté sur un axone, qui se
sépare à son extrémité en plusieurs branches se terminant par une (ou plusieurs) synapse(s). 

Les synapses sont les canaux d'entrée des neurones. 
Chaque synapse traite un signal $s_i^\text{in}$ émis par un autre neurone pré-synaptique $i$ du réseau et produit une \textit{entrée synaptique} $e(s_i^\text{in}$).
L'état \textit{interne} du neurone post-synaptique est donné par son \textit{potentiel de membrane} $V$.
La somme des entrées synaptiques agit sur la valeur de ce potentiel de membrane. 
L'intégration de la totalité des entrées peut être exprimée par une fonction de mise à jour du potentiel de membrane de la forme:
\begin{align}\label{eq:neurone_V}
V = g(s_1^\text{in}, ..., s_n^\text{in})
\end{align}
où $s_1^\text{in}, ..., s_n^\text{in}$ sont les $n$ signaux entrants.

\emph{Exemples:}
\begin{itemize}
\item Dans le cas le plus simple, $g$ est une combinaison linéaire des entrées synaptiques \shortcite{MCu43,Rosen58}.
\item Les modèles plus détaillés prennent en compte 
la fonction de transfert des synapses ainsi que les interactions non-linéaires entre les influences excitatrices et les influences inhibitrices \shortcite{DESTEXHE97}.
\end{itemize}

L'activation d'un neurone repose ensuite sur un mécanisme non linéaire de \textit{passage de seuil}.

\emph{Exemples:}
\begin{itemize}
\item Dans le modèle le plus simple \shortcite{MCu43}, l'état de sortie (0 ou 1) dépend d'un
seuil d'activation $\theta$ tel que $s=1$ (état haut) si $V>\theta$ et $s=0$ (état bas) sinon.
\item
Dans les modèles plus détaillés \shortcite{Lapicque1907,HH52}, le passage à l'état haut déclenche un mécanisme actif de réinitialisation qui ramène
le potentiel de membrane vers sa valeur de repos $V_0 < \theta$.  Ce mécanisme de réinitialisation, qui interdit
le maintien de la sortie dans l'état haut, rend l'état haut  
plus rare (et donc plus porteur d'information) que l'état bas. Il a également pour effet d'effacer 
la mémoire des données d'entrée antérieures au dernier potentiel d'action.
\end{itemize}


%% A mettre dans la partie sur la plasticité
%Un des modèles les plus étudiés en modélisation est le modèle de l'\textit{intégrateur à fuite}. 
%La valeur du potentiel dépend de l'historique des signaux d'entrée depuis le dernier PA jusqu'à l'instant $t$. 
%{\bf !! TODO  mécanisme de décision basé sur l'accumulation + figure!!}.
% La partie suivante doit commencer avec les modèles de la décision : décision bayésienne, intégrateur à seuil, décision de Hopfield,
% décision du champ neuronal, avec pour finir le modèle de Pouget (compromis dynamique)
% Préciser ensuite le mécanisme de sélection/intégration

\subsection{Réseau de neurones}

Plusieurs neurones connectés entre eux via des axones constituent 
un \textit{réseau de neurones}. Le schéma de connexion, sous la forme d'un graphe orienté, 
caractérise l'\textit{organisation structurelle} du réseau. 
%Selon que le graphe est acyclique, ou au contraire récurrent, .

La description du réseau est donné par une \textit{fonction de couplage},
définie le plus souvent par un tableau à double entrée $T$. Chaque neurone
se voit attribuer un indice $i \in 1,...,n$ où $n$ est le nombre de neurones dans le réseau. Le tableau
$T[i,j]$ contient alors les caractéristiques du lien du neurone $i$ vers le neurone $j$.
Un exemple de paramétrisation est le temps de propagation $\tau_{ij}$ et poids synaptique $J_{ij}$. 
Lorsque le poids synaptique est non-nul, on dit que les neurones $i$ et $j$ sont couplés. 
Si $n$ est le nombre de neurones, le nombre de paramètres du réseau est en $O(n^2)$, correspondant à un principe de transmission du signal \textit{de plusieurs à plusieurs} (un neurone envoie son signal de sortie vers plusieurs destinataires, un neurone reçoit des signaux de plusieurs émetteurs).

\emph{Notations:}
\begin{itemize} 
\item Le \textit{vecteur d'activité} $\boldsymbol{s}(t)$ (ou activité de population) est un vecteur de taille $n$ contenant l'ensemble des 
sorties à l'instant $t$~: 
$$\boldsymbol{s}(t) = (s_1^\text{out}(t), ..., s_n^\text{out}(t))$$
\item Le \textit{patron d'activité} est constituée de l'ensemble des signaux émis entre l'instant initial $t_0$  et l'instant d'observation $t$, soit: $$\boldsymbol{S}(t)=\{\boldsymbol{s}(t')\}_{t' \in [t_0, t[}$$
\end{itemize}

Les activités des différents neurones sont donc \textit{inter}dépendantes du fait des couplages. 
La \textit{fonction de réponse} du réseau $f_T$ est une fonction paramétrique 
décrite par le tableau de paramètres $T$, telle que l'activité $\boldsymbol{s}(t)$ est solution de~:
\begin{align}\label{eq:optim}
\boldsymbol{s}(t) = f_T(\boldsymbol{S}(t))
\end{align}
Il faut ici préciser, dans la mesure où les temps de transports sont supposés strictement positifs, que l'activité du réseau au temps $t$ est
dépendante de l'historique des activités précédent strictement 
l'instant $t$. L'activité d'un neurone post-synaptique $i$ au temps $t$ est dépendante des signaux pré-synaptiques
produits aux instants $t' < t$, en tenant compte du \textit{temps de transport} de ces signaux sur les axones. Plus précisément, si $j$ est le 
neurone pré-synaptique et $i$ le neurone post-synaptique, on a:
\begin{align}\label{eq:transport}
s^\text{in}_{ij}(t) = s^\text{out}_{j}(t - \tau_{ij})
\end{align}

\subsection{Architecture de réseau}

On parle d'\textit{architecture de réseau} pour décrire les propriétés générales du graphe.
L'architecture définit généralement le nombre de couches, le type de connexions de couche à couche, 
et éventuellement le nombre de neurones par couche.
Un exemple typique d'architecture est l'organisation séquentielle des réseaux de neurones dits
``à couche cachée'',
qu'on trouve dans les perceptrons multi-couches, les réseaux profonds, etc. [CITATIONS] 
Un autre exemple d'architecture est celle des mémoires associatives, constituées d'une ou plusieurs couches de 
neurones connectés sous la forme d'un circuit récurrent (ou réentrant) [CITATION].

{\color{Cyan} Notion de faisceau d'axones. Construction de réseaux.}

\subsection{Calcul distribué}

Les signaux transitant de neurone à neurone via les axones du réseau contiennent une succession d'états hauts et
d'états bas qui présentent une ressemblance  avec les signaux digitaux produits par les calculateurs 
numériques. Cette analogie de forme n'implique cependant pas que les principes de traitement et
de transformation de ces signaux soient les mêmes.

Un calcul est classiquement décrit comme une séquence d'opérations élémentaires réalisées dans un certain ordre
à partir de données d'entrée. Ces opérations élémentaires peuvent être réalisées par les portes logiques
d'un circuit électronique, ou par tout mécanisme automatique capable de ségréger ses entrées sous la forme d'au moins 
deux états de sortie distincts. 
%La généralisation 
%aux calculateurs universels se fait à travers la présence de mémoires, c'est à dire d'unités capables de
%conserver un état (haut ou bas) pour une durée indéterminée. 

Dans la cas des réseaux de neurones, la mise en oeuvre d'un calcul repose sur la présence
de neurones d'\textit{entrée} soumis à des données extérieures, qui sont donc les opérandes du calcul.
C'est le cas chez l'animal des cellules \textit{sensorielles}. 
On note $\boldsymbol{I}(t) = \{\boldsymbol{I}(t')\}_{t' \in [t_0, t[}$ ce signal extérieur. 
Si le neurone $i$ est un neurone d'entrée, on a $I_i \neq 0$. Si le neurone $i$ n'est pas un neurone d'entrée,  
on a $I_i = 0$.
Le signal extérieur est généralement considéré comme indépendant de l'activité du réseau de neurones.

La \textit{réponse} du réseau de neurones au signal d'entrée (la solution du calcul) est le patron d'activité induit
 par ce signal d'entrée.
En étendant la fonction de réponse aux entrées extérieures, l'activité $\boldsymbol{s}(t)$ est la solution de~:
\begin{align}\label{eq:optim_input}
\boldsymbol{s}(t) = f_T(\boldsymbol{S}(t), \boldsymbol{I}(t))
\end{align}
On dit également 
que le réseau de neurones \textit{transforme} le signal d'entrée.

Il est possible de définir une \textit{fonction de sortie} qui \textit{décode} ce patron d'activité. 
La sortie du réseau est alors 
$$\boldsymbol{u} = h(\boldsymbol{s})$$
avec $h$ fonction de décodage (ou de ``\textit{read-out}'').

Plus globalement, la fonction d'entrée/sortie (fonction de transfert) du réseau de neurones est~:
\begin{align}\label{eq:E/S}
 \boldsymbol{u} = h(f_T(\boldsymbol{S},\boldsymbol{I}))
\end{align}

La sortie $\boldsymbol{u}$ peut ainsi être interprétée comme le résultat du calcul réalisé à partir des données d'entrée $\boldsymbol{I}$. 
Dans la mesure où ce résultat est le produit de 
l'activité conjointe (et parallèle) des neurones du réseau, on se situe dans un contexte de \textit{calcul distribué}, par opposition au calcul
séquentiel centralisé réalisé par les ordinateurs traditionnels.

Il est souvent possible de regrouper différents patrons d'activité selon des caractéristiques communes
se traduisant par une valeur unique au niveau de la fonction de sortie $h$. 
\begin{itemize}
\item On parlera des différents \textit{modes de sortie} du réseau de neurones.
\item L'\textit{encodage} est le processus par lequel un jeu a priori non borné de signaux 
d'entrée se traduira sous la forme d'un nombre fini de modes de sortie.
Chaque mode est alors le reflet, au sein du réseau, d'une \textit{classe} de signaux. 
\item On parlera d'\textit{expressivité} pour désigner le nombre et la nature des modes de sortie produits par un réseau de neurones,
en présence ou en l'absence de signal extérieur. 
Le nombre et la nature de ces modes
nous permettra de décrire son ``vocabulaire'', ainsi que le type d'opérations dont il peut être le support. 
\end{itemize}

\subsection{Réseaux de neurones et apprentissage automatique}

La programmation des réseaux de neurones est un des piliers de la recherche 
en apprentissage automatique (ou ``\textit{Machine learning}''). 
Le but d'un algorithme d'apprentissage est de définir 
le jeu de paramètres $T$ de manière à produire une sortie 
conforme au calcul souhaité. 
Le jeu de paramètres est assimilables à un \textit{programme} et
le choix de ces paramètres correspond à la programmation du réseau. 

Le principe de l'apprentissage automatique est de confier cette tâche 
de programmation à un algorithme, dit algorithme d'apprentissage. 
En d'autres termes, il faut écrire un programme capable de programmer le réseau de neurones. 
Ce \textit{méta-programme} s'appuie sur des méta-données qui sont 
des données servant à produire le programme.
Cette conception de la programmation basée sur l'optimisation
d'un tableau de paramètres via un ensemble de contraintes
remonte historiquement au principes de la programmation dynamique \shortcite{Bellman1956}.

Dans le cadre de l'apprentissage automatique, 
les réseaux de neurones sont principalement utilisés pour
des tâches de catégorisation de leurs données d'entrée.
Les méta-données sont alors constituées 
d'un jeu de données d'entrée ainsi que des sorties attendues,
appelé \textit{base d'apprentissage}.

% programme = paramètres
% algorithme = meta programme

Les modèles de réseaux de neurones, % (encore dite approche ``distribuée''), 
tels qu'il se sont développés depuis le perceptron \shortcite{Rosen58}, reposent majoritairement sur
un principe de traitement séquentiel (``\textit{feed forward}'')
[REFERENCES : PMC, Vapnik, LeCun, ].
%une notion implicite de ressource limitée.


\begin{itemize}
 \item Le modèle de perceptron le plus simple, possédant une cellule de sortie unique, sépare son espace d'entrée en deux régions.
Les données provenant de la première région produisent une activité de sortie ``haute'', qui s'apparente
au mode ``actif''. Les données provenant 
de la seconde région produisent une une activité de sortie ``basse'', qui s'apparente au mode ``inactif''.
 \item Dans le cas du perceptron multi-couches [CITATION],
chaque neurone de la couche cachée est un séparateur permettant de distinguer deux régions de son espace d'entrée. 
Le nombre de régions séparables (le nombre maximal de modes de sortie) augmente exponentiellement avec le nombre de neurones dans la couche  
couche cachée.
Le concepteur fixe à l'avance le nombre de neurones à mettre dans la couche cachée,
et donc la complexité des opérations de ségrégation produites par le réseau.
\item Enfin, le modèle des séparateurs à vaste marge (SVM) \shortcite{Vap95}
propose une définition quantitative de la complexité d'un classifieur~: la
dimension de Vapnik-Chervonenkis, qui correspond au nombre de pivots utilisé pour
séparer les données d'entrée. 
\end{itemize}


\subsection{Réseaux de neurones et modélisation biologique}

L'inspiration biologique conduit à étudier des modèles alternatifs à ce traitement
séquentiel standard. 
Dans le cadre de ce mémoire, nous regarderons en particulier 
l'idée de \textit{circuit reconfigurable}, basée sur l'existence d'une \textit{activité endogène} produite par un réseau de 
neurones \textit{récurrent}.  
Le but est de caractériser ces mécanismes, et 
d'identifier les points communs avec  
des mécanismes similaires à l'{\oe}uvre dans le cerveau. 

Les réseaux de neurones récurrents sont définis par le fait que leur graphe contient des cycles.
Ces cycles conduisent à des dépendances réciproques entre les unités du réseau. 
Le formalisme des systèmes dynamiques permet de décrire ces interactions croisées et leur effet sur les patrons
d'activité produits par le réseau.

\paragraph{Quelques notions de la théorie des systèmes dynamiques}
L'activité du réseau au temps $t$ est le résultat de l'intégration des
potentiels d'action émis aux instants précédant $t$, en tenant compte des délais de transmission, et de la valeur courante du signal externe
(voir eq. (\ref{eq:optim_input})).

La théorie des \textit{systèmes dynamiques} repose sur un espace d'état $\mathcal{X}$, une trame temporelle 
$\mathcal{T}$ et un
flot $\phi$ qui est une application de $\mathcal{X} \times \mathcal{T}$ dans $\mathcal{X}$
définissant pour tout couple $(x,t)$ la valeur de l'état suivant:
\begin{align}\label{eq:SD}
x' = \phi(x,t)
\end{align}
La plupart des réseaux de neurones peuvent se modéliser sous cette forme, à partir du moment où 
l'état du réseau à l'instant $t$ est entièrement spécifié, en tenant compte en particulier
du potentiel de membrane et 
des différents temps de transport sur les axones. 

La \textit{trajectoire} du système sur la plage temporelle $[t_0,t_f]$ est alors définie 
par l'intégration sur le flot de la condition initiale $\boldsymbol{x}_0$.

\begin{itemize}
\item Dans le cas d'un système dit ``autonome'', on a $\phi(x,t) = \phi(x)$ et
la trajectoire du système $\{x(t)\}_{t \in [t_0,t_f], \boldsymbol{x}(t_0)= \boldsymbol{x}_0}$ 
est entièrement définie par les conditions initiales. 
\item Dans le cas d'un système dit ``non-autonome'', la dépendance temporelle
est souvent modélisée sous la forme d'un signal externe $I(t)$, soit $\phi(x,t) = \phi(x,I(t))$. 
Ce signal peut être:
  \begin{itemize}
  \item une donnée d'entrée dans le cas de modèles de traitement des données
  \item un bruit externe dans le cas de modèles stochastiques [REF?]
  \end{itemize}
\end{itemize}

\paragraph{}
Dans le cas autonome...
{\color{Cyan} Notion d'attracteur}

\paragraph{}
Dans le cas non autonome, la trajectoire dépend à la fois des conditions initiales et du signal exterieur. 
\begin{itemize}
\item On parle dans ce cas d'une activité \textit{induite} par le signal d'entrée. 
\item Du point de vue 
computationnel, celà signifie que le même signal peut recevoir un traitement différent si les conditions initiales sont différentes. 
Sans précision des conditions initiales, de multiples opérations sont donc possibles pour un même signal d'entrée. 
On parlera de différents \textit{modes d'opération}.
\end{itemize}


En pratique, l'équation (\ref{eq:SD}) prend souvent la forme d'une équation différentielle [REFS]
ou intégro-différentielle [REFS] qui se résout
par des méthodes numériques. 


%Dans un cadre continu, l'équation d'évolution est la suivante~:
%$$ \frac{d\boldsymbol{x}}{dt} = \Phi(\boldsymbol{x},t)$$
%Dans un cadre discret, l'équation d'évolution est~:
%$$ \boldsymbol{x}(t+1) = \Phi(\boldsymbol{x}(t),t)$$

%où $\Phi(\boldsymbol{x},t)$, où $t \in \mathcal{T}$ décrit un instant
%situé sur la trame temporelle et la variable d'état $\boldsymbol{x} \in \mathcal{X}$ décrit
%l'état du système.

\emph{Exemples:}
\begin{itemize}
\item Un exemple minimaliste est un réseau de neurones récurrent à deux neurones mutuellement connectés. 
Un tel réseau pourrait présenter deux modes : un mode ``inactif'' dans lequel les deux neurones
sont dans l'état ``bas'', et un mode ``actif'' dans lequel une activité ``haute'' locale est transmise
indéfiniment d'un neurone à l'autre. Chacun de ces deux patrons d'activité est une solution 
du graphe de contraintes. 
%\item 

\item {\color{Orange} L'exemple des réseaux de Hopfield \shortcite{Hop82} illustre bien la notion de modes multiples. 
%offre  un éclairage différent. 
%sur l'expressivité d'un substrat en mettant en 
%avant le caractère distribué (non-local) de l'activité. 
%Contrairement à l'idée classique d'un noeud de sortie comme unité d'expression, 
%(par exemple unité de séparation dans le cas des classifieurs, 
%unité de description dans le cas des cartes auto-organisées etc.), 
L'expressivité du réseau est vue sous l'angle du nombre
de patrons distribués (patterns) pouvant être obtenus en tant qu'état final (attracteur) atteint par le système
dynamique décrit par les poids (arêtes) du réseau.
La capacité s'exprime ici en nombre de \emph{modes} (configurations atteignables) 
et non en nombre de noeuds/vecteurs supports etc.
Si la capacité se trouve en principe augmentée
d'un facteur exponentiel ($2^N$ configurations distribuées possibles), 
la capacité effective est en pratique beaucoup plus réduite, 
se limitant à un nombre de modes (attracteurs) distincts en $O(N)$, 
c'est à dire de l'ordre du nombre de noeuds \shortcite{Amit1987,Tso88}.
La nature des opérations réalisables par le substrat est également différent puisqu'il s'agit principalement ici d'une
dynamique de relaxation adaptée à des opération de complétion et d'interpolation de données manquantes, implémentant un
mécanisme de mémoire dite ``auto-associative''. }
\item 

\end{itemize}


Lorsque le système est dissipatif, le flot définit un ensemble ... 

Notion d'attrateur 

Dans le cadre des études inspirées par la bioologe, il est fréquent d'utiliser des modèles plus simples
appelés modèles de \textit{champ moyen}.

Nous regardons en particulier le cas des réseaux de neurones 
récurrents aléatoires [A CONNECTER AU RESTE].

\section{Grands réseaux aléatoires}

{\color{Gray} \subsection{Systèmes dynamiques}


OBJECTIF : 

Ce formalisme est utilisé par exemple 

Les modèle qui servent de fondement aux neurosciences computationnelles 
sont les modèles de neurones à conductance [REF DESTEXHE] dont le plus connu est le modèle de Hudgkin-Huxley. 

qui établit 

Lorsque le réseau est récurrent 


notion d'état du réseau, et de patron d'activité

axe temporel, système dynamique et conditions initiales.

équation d'évolution de la dynamique



... neurones de sortie et read-out...

... circuit de traitement des données ...
mes. 

}

\subsection{Champ moyen}

\paragraph{Taux de décharge instantané} L'activité des neurones telle que mesurée par électrophysiologie présente
un caractère irrégulier où les temps d'émission des PA semblent obéir à un processus de Poisson [CITATION].
Un tel processus est caractérisé par sa fréquence instantanée $\nu$, qui donne le nombre de PA émis par seconde.
La grandeur $\nu$ peut être mesurée 
à travers l'observation d'un train de PA produit par un neurone particulier (nombre de PA émis divisé par
l'intervalle d'observation).

De nombreux modèles stochastiques de neurones ont été proposés qui prennent en compte le caractère aléatoire de la 
réponse des neurones biologiques. 


\paragraph{Activité de population}
Le modèle de champ moyen considère l'activité instantanée de populations de neurones $\nu(t)$, qui exprime, à un instant
donné, le nombre de neurones émettant un PA divisé par la taille de la population.
Un interprétation commune de l'activité des

\bibliographystyle{mslapa}%{apalike}%{apacite}

\bibliography{biblio}
\end{document}

